{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096c868b-ef91-42df-903f-a0a046ccad95",
   "metadata": {},
   "source": [
    "# Solving a Brax environment using EvoTorch\n",
    "\n",
    "This notebook demonstrates how the Brax environment named `humanoid` can be solved using EvoTorch.\n",
    "\n",
    "EvoTorch provides `VecGymNE`, a neuroevolution problem type that focuses on solving vectorized environments. If GPU is available, `VecGymNE` can utilize it to boost performance. In this notebook, we use `VecGymNE` to solve the `humanoid` task.\n",
    "\n",
    "For this notebook to work, the libraries [JAX](https://jax.readthedocs.io/en/latest/) and [Brax](https://github.com/google/brax) are required.\n",
    "For installing JAX, you might want to look at its [official installation instructions](https://github.com/google/jax#installation).\n",
    "After a successful installation of JAX, Brax can be installed via:\n",
    "\n",
    "```\n",
    "pip install brax\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Below, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c786f423-220e-4c86-8e06-05edbfd45c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evotorch.algorithms import PGPE\n",
    "from evotorch.neuroevolution import VecGymNE\n",
    "from evotorch.logging import StdOutLogger, PicklingLogger\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b547977-1478-4b82-95c4-12a79769336d",
   "metadata": {},
   "source": [
    "We now check if CUDA is available. If it is, we prepare a configuration which will tell `VecGymNE` to use a single GPU both for the population and for the fitness evaluation operations. If CUDA is not available, we will instead turn to actor-based parallelization on the CPU to boost the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45edbf5-b0e9-43a0-869a-5146321ddd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # CUDA is available. Here, we prepare GPU-specific settings.\n",
    "    \n",
    "    # We tell XLA (the backend of JAX) to use half of a GPU.\n",
    "    os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".5\"\n",
    "    \n",
    "    # We make only one GPU visible.\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    \n",
    "    # This is the device on which the population will be stored\n",
    "    device = \"cuda:0\"\n",
    "    \n",
    "    # We do not want multi-actor parallelization.\n",
    "    # For most basic brax tasks, it is enough to use a single GPU both\n",
    "    # for the population and for the solution evaluations.\n",
    "    num_actors = 0\n",
    "else:\n",
    "    # Since CUDA is not available, the device of the population will be cpu.\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    # Use all the CPUs to speed-up the evaluations.\n",
    "    num_actors = \"max\"\n",
    "    \n",
    "    # Because we are already using all the CPUs for actor-based parallelization,\n",
    "    # we tell XLA not to use multiple threads for its operations.\n",
    "    # (Following the suggestions at https://github.com/google/jax/issues/743)\n",
    "    os.environ[\"XLA_FLAGS\"] = \"--xla_cpu_multi_thread_eigen=false intra_op_parallelism_threads=1\"\n",
    "\n",
    "    # We also tell OpenBLAS and MKL to use only 1 thread for their operations.\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1387654-13b4-4367-9d3e-ffda6b3aaf5d",
   "metadata": {},
   "source": [
    "We now define our policy. The policy can be expressed as a string, or as an instance or as a subclass of `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62b5e2-6471-4c86-b028-7997426270f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A simple linear policy ---\n",
    "policy = \"Linear(obs_length, act_length)\"\n",
    "\n",
    "\n",
    "# --- A feed-forward network ---\n",
    "# policy = \"Linear(obs_length, 64) >> Tanh() >> Linear(64, act_length)\"\n",
    "\n",
    "\n",
    "# --- A feed-forward network with layer normalization ---\n",
    "# policy = (\n",
    "#     \"\"\"\n",
    "#     Linear(obs_length, 64)\n",
    "#     >> Tanh()\n",
    "#     >> LayerNorm(64, elementwise_affine=False)\n",
    "#     >> Linear(64, act_length)\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# --- A recurrent network with layer normalization ---\n",
    "# Note: in addition to RNN, LSTM is also supported\n",
    "#\n",
    "# policy = (\n",
    "#     \"\"\"\n",
    "#     RNN(obs_length, 64)\n",
    "#     >> LayerNorm(64, elementwise_affine=False)\n",
    "#     >> Linear(64, act_length)\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "\n",
    "# --- A manual feed-forward network ---\n",
    "# class MyManualNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         ...\n",
    "#\n",
    "#    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#        ...\n",
    "#\n",
    "# policy = MyManualNetwork\n",
    "\n",
    "\n",
    "# --- A manual recurrent network ---\n",
    "# class MyManualRecurrentNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         ...\n",
    "#\n",
    "#     def forward(self, x: torch.Tensor, hidden_state = None) -> tuple:\n",
    "#         ...\n",
    "#         output_tensor = ...\n",
    "#         new_hidden_state = ...  # hidden state could be a tensor, or a tuple or dict of tensors\n",
    "#         return output_tensor, new_hidden_state\n",
    "#\n",
    "# policy = MyManualRecurrentNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e7f59-b727-44b4-9f60-dc7cbec9943b",
   "metadata": {},
   "source": [
    "Below, we instantiate our `VecGymNE` problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5f554-cdec-40a5-8e21-fa09ea53e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"brax::humanoid\"  # solve the brax task named \"humanoid\"\n",
    "# ENV_NAME = \"brax::old::humanoid\"  # solve the \"humanoid\" task defined within 'brax.v1`\n",
    "\n",
    "problem = VecGymNE(\n",
    "    env=ENV_NAME,\n",
    "    network=policy,\n",
    "    #\n",
    "    # Collect observation stats, and use those stats to normalize incoming observations\n",
    "    observation_normalization=True,\n",
    "    #\n",
    "    # In the case of the \"humanoid\" task, the agent receives an \"alive bonus\" of 5.0 for each\n",
    "    # non-terminal state it observes. In this example, we cancel out this fixed amount of\n",
    "    # alive bonus using the keyword argument `decrease_rewards_by`.\n",
    "    # The amount of alive bonus changes from task to task (some of them don't have this bonus\n",
    "    # at all).\n",
    "    decrease_rewards_by=5.0,\n",
    "    #\n",
    "    # As an alternative to giving a fixed amount of alive bonus, we now enable a scheduled\n",
    "    # alive bonus.\n",
    "    # From timestep 0 to 400, the agents will receive no alive bonus.\n",
    "    # From timestep 400 to 700, the agents will receive partial alive bonus.\n",
    "    # Beginning with timestep 700, the agents will receive full (10.0) alive bonus.\n",
    "    alive_bonus_schedule=(400, 700, 10.0),\n",
    "    device=device,\n",
    "    num_actors=num_actors,\n",
    ")\n",
    "\n",
    "problem, problem.solution_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce02d7c-400c-4c22-9bb8-aa70fa4b1da2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note.**\n",
    "At the time of writing this (15 June 2023), the [arXiv paper of EvoTorch](https://arxiv.org/abs/2302.12600v3) reports results based on the old implementations of the brax tasks (which were the default until brax v0.1.2). In brax version v0.9.0, these old task implementations moved into the namespace `brax.v1`. If you wish to reproduce the results reported in the arXiv paper of EvoTorch, you might want to specify the environment name as `\"brax::old::humanoid\"` (where the substring `\"old::\"` causes `VecGymNE` to instantiate the environment using the namespace `brax.v1`), so that you will observe scores and execution times compatible with the ones reported in that arXiv paper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95417793-3835-47b1-b10a-7f36e78fa3ad",
   "metadata": {},
   "source": [
    "Initialize a PGPE to work on the problem.\n",
    "\n",
    "Note: If you receive memory allocation error from the GPU driver, you might want to try again with:\n",
    "- a decreased `popsize`\n",
    "- a policy with decreased hidden size and/or number of layers (in case the policy is a neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9f851-68aa-4e67-9dbb-2474a5ebd4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS = 2.25\n",
    "MAX_SPEED = RADIUS / 6\n",
    "CENTER_LR = MAX_SPEED / 2\n",
    "\n",
    "# Instantiate a PGPE using the hyperparameters prepared above\n",
    "searcher = PGPE(\n",
    "    problem,\n",
    "    popsize=12000,\n",
    "    radius_init=RADIUS,\n",
    "    center_learning_rate=CENTER_LR,\n",
    "    optimizer=\"clipup\",\n",
    "    optimizer_config={\"max_speed\": MAX_SPEED},\n",
    "    stdev_learning_rate=0.1,\n",
    ")\n",
    "\n",
    "searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da60f156-6756-41a4-b261-82ee62d7f7cb",
   "metadata": {},
   "source": [
    "We register two loggers for our PGPE instance.\n",
    "\n",
    "- **StdOutLogger:** A logger which will print out the status of the optimization.\n",
    "- **PicklingLogger:** A logger which will periodically save the latest result into a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91270ba2-ce78-43e7-bf01-20c94b0529c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = StdOutLogger(searcher)\n",
    "pickler = PicklingLogger(searcher, interval=5, directory=\"humanoid_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d16c1-078c-4d7d-bd6f-e3ff28173667",
   "metadata": {},
   "source": [
    "We are now ready to start the evolutionary search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a84a0-47ea-4592-bd37-5e96fc8f6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.run(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491f968-4f43-4df6-aac0-a09c756185da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The run is finished.\")\n",
    "print(\"The pickle file that contains the latest result is:\")\n",
    "print(pickler.last_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731e007-55b3-49ef-9285-9d9137232c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickler.unpickle_last_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa9df1-c978-4c2a-a528-c98e761caec7",
   "metadata": {},
   "source": [
    "Now, we receive our trained policy as a torch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4d211-08c2-4a59-ab84-23988646895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_solution = searcher.status[\"center\"]\n",
    "policy = problem.to_policy(center_solution)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7581d-24ba-4f06-88e2-41bb3274cd37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualizing the trained policy\n",
    "\n",
    "Now that we have our final policy, we manually run and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f0ea4-7638-4e8a-a0a2-d9f483c8c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "import brax\n",
    "\n",
    "if ENV_NAME.startswith(\"brax::old::\"):\n",
    "    import brax.v1\n",
    "    import brax.v1.envs\n",
    "    import brax.v1.jumpy as jp\n",
    "    from brax.v1.io import html\n",
    "    from brax.v1.io import image\n",
    "else:\n",
    "    try:\n",
    "        import jumpy as jp\n",
    "    except ImportError:\n",
    "        import brax.jumpy as jp\n",
    "    import brax.envs\n",
    "    from brax.io import html\n",
    "    from brax.io import image\n",
    "\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d38fe1-9d6f-40ec-926d-3c4066a4b66a",
   "metadata": {},
   "source": [
    "Below, we define a utility function named `use_policy(...)`.\n",
    "\n",
    "The expected arguments of `use_policy(...)` are as follows:\n",
    "\n",
    "- `torch_module`: The policy object, expected as a `nn.Module` instance.\n",
    "- `x`: The observation, as an iterable of real numbers.\n",
    "- `h`: The hidden state of the module, if such a state exists and if the module is recurrent. Otherwise, it can be left as None.\n",
    "\n",
    "The return values of this function are as follows:\n",
    "\n",
    "- The action recommended by the policy, as a numpy array\n",
    "- The hidden state of the module, if the module is a recurrent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6554d-4bdb-49b9-8c2e-956bce6ddb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def use_policy(torch_module: nn.Module, x: Iterable, h: Optional[Iterable] = None) -> tuple:\n",
    "    x = torch.as_tensor(np.array(x), dtype=torch.float32)\n",
    "    if h is None:\n",
    "        result = torch_module(x)\n",
    "    else:\n",
    "        result = torch_module(x, h)\n",
    "\n",
    "    if isinstance(result, tuple):\n",
    "        x, h = result\n",
    "        x = x.numpy()\n",
    "    else:\n",
    "        x = result.numpy()\n",
    "        h = None\n",
    "        \n",
    "    return x, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c044fe-4639-411c-be78-ac09cbe5e78f",
   "metadata": {},
   "source": [
    "We now initialize a new instance of our brax environment, and trigger the jit compilation on its `reset` and `step` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1251e82-1d0f-4e43-a6c5-1ec4c0275dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENV_NAME.startswith(\"brax::old::\"):\n",
    "    env = brax.v1.envs.create(env_name=ENV_NAME[11:])\n",
    "else:\n",
    "    env = brax.envs.create(env_name=ENV_NAME[6:])\n",
    "\n",
    "reset = jax.jit(env.reset)\n",
    "step = jax.jit(env.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55229cc2-aad5-4c78-b095-010a538adb40",
   "metadata": {},
   "source": [
    "Below we run our policy and collect the states of the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ec837-1d1a-401e-a144-5516bdb3e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.randint(0, (2 ** 32) - 1)\n",
    "\n",
    "if hasattr(jp, \"random_prngkey\"):\n",
    "    state = reset(rng=jp.random_prngkey(seed=seed))\n",
    "else:\n",
    "    state = reset(rng=jax.random.PRNGKey(seed=seed))\n",
    "\n",
    "h = None\n",
    "states = []\n",
    "cumulative_reward = 0.0\n",
    "\n",
    "while True:\n",
    "    action, h = use_policy(policy, state.obs, h)\n",
    "    state = step(state, action)\n",
    "    cumulative_reward += float(state.reward)\n",
    "    states.append(state)\n",
    "    if np.abs(np.array(state.done)) > 1e-4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bbe53a-40f5-4bf5-a063-47f1d88032d6",
   "metadata": {},
   "source": [
    "Length of the episode and the total reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80345e9c-0694-413d-81c2-205dfacdfb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(states), cumulative_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ec424-e6cc-453a-93fd-eb07e44c1bd6",
   "metadata": {},
   "source": [
    "Visualization of the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec60419-1ad0-4f19-bc1e-f2048577ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENV_NAME.startswith(\"brax::old::\"):\n",
    "    env_sys = env.sys\n",
    "    states_to_render = [state.qp for state in states]\n",
    "else:\n",
    "    env_sys = env.sys.replace(dt=env.dt)\n",
    "    states_to_render = [state.pipeline_state for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c70f6-2c93-43a1-b4c3-edd3f395302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(html.render(env_sys, states_to_render))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
